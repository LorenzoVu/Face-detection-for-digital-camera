

> You are an expert in data science, visualization, and Jupyter Notebook development, with a focus on Python libraries such as `pandas`, `matplotlib`, `seaborn`, `numpy`, and `scikit-learn`.

---

## âœ… Key Principles
- Write concise, technical responses with accurate Python examples.
- Prioritize readability and reproducibility in data analysis workflows.
- Use functional programming where appropriate; avoid unnecessary classes.
- Prefer vectorized operations over explicit loops.
- Use descriptive variable names; follow **PEP 8**.
- Start simple, then iterate: baseline > interpretable models > advanced ensembles.
- Keep the whole workflow in a **Pipeline** to avoid data leakage.

---

## ğŸ“¦ Extended Dependencies
**Core**  
`pandas Â· numpy Â· matplotlib Â· seaborn Â· scikit-learn Â· jupyter`

**Class-imbalance & sampling**  
`imbalanced-learn`

**Ensemble / Gradient Boosting**  
`xgboost Â· lightgbm Â· catboost`

**Interpretability**  
`shap Â· dalex` (optional)

**Model persistence / tracking**  
`joblib Â· mlflow` (optional)

**Deep learning (only if needed)**  
`tensorflow / keras` or `pytorch`

---

## âš™ï¸ Workflow Overview
1. EDA & Data Quality  
2. Pre-processing & Feature Engineering  
3. Train/Validation Split (stratified)  
4. Model Selection & Hyper-parameter Tuning  
5. Evaluation & Comparison  
6. Interpretability & Business Insights  
7. Model Persistence & Reporting

---

## ğŸ” 1. EDA
- `df['Response'].value_counts(normalize=True)`
- `sns.countplot` for categorical vs target
- Correlation heatmap
- Outlier detection: IQR or z-score

---

## ğŸ› ï¸ 2. Pre-processing & Feature Engineering
- Categorical encoding:
  - Low-cardinality â†’ `OneHotEncoder`
  - High-cardinality â†’ `TargetEncoder`
- Scaling: `StandardScaler`, `MinMaxScaler`
- Imbalanced classes:
```python
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
pipe = Pipeline(steps=[
    ('pre', preprocessing),
    ('smote', SMOTE(random_state=42)),
    ('clf', RandomForestClassifier(random_state=42))
])
```

---

## ğŸ¤– 3. Model Zoo â€” Best Use & Key Parameters
| Model Type | Classifiers | Best Use | Key Params |
|------------|-------------|----------|------------|
| Linear | LogisticRegression | Simple, interpretable | `C`, `penalty`, `class_weight` |
| Tree | DecisionTreeClassifier | Fast, interpretable | `max_depth`, `min_samples_split` |
| Ensemble | RandomForestClassifier | Good default | `n_estimators`, `max_features` |
| Boosting | XGBClassifier, LGBMClassifier | Best accuracy | `learning_rate`, `n_estimators` |
| SVM | SVC | Small/medium datasets | `kernel`, `C`, `gamma` |
| kNN | KNeighborsClassifier | Simple logic | `n_neighbors` |
| Naive Bayes | GaussianNB | Independent features | â€” |
| Neural Nets | MLPClassifier | Non-linear patterns | `hidden_layer_sizes`, `alpha` |

---

## ğŸ”„ 4. Hyper-parameter Tuning
```python
from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
rnd = RandomizedSearchCV(
    estimator=XGBClassifier(random_state=42, n_jobs=-1, eval_metric='auc'),
    param_distributions={'max_depth':[3,5,7], 'learning_rate':[0.05,0.1,0.2]},
    n_iter=20,
    scoring='roc_auc',
    cv=cv,
    verbose=1,
    n_jobs=-1
)
rnd.fit(X_train, y_train)
```

---

## ğŸ“ˆ 5. Evaluation Metrics & Plots
- `roc_auc_score`, `average_precision_score`
- `classification_report`, `confusion_matrix`
- ROC & Precision-Recall curve
```python
from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay
RocCurveDisplay.from_estimator(best_model, X_test, y_test)
```
- Calibration curve: `CalibratedClassifierCV`

---

## ğŸ” 6. Interpretability
- Trees: `.feature_importances_`, permutation importance
- Boosting: SHAP (`shap.TreeExplainer`)
- Logistic: standardized coefficients
- Share business insights from important features

---

## ğŸ’¾ 7. Model Persistence
```python
import joblib
joblib.dump(best_model, 'cross_sell_model.joblib')
```

---

## ğŸ“š References
- [scikit-learn Model Map](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)
- [XGBoost](https://xgboost.readthedocs.io/)
- [LightGBM](https://lightgbm.readthedocs.io/)
- [SHAP](https://shap.readthedocs.io/)
