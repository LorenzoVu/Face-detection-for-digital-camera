# Project Execution â€“ Faceâ€‘Detection Pipeline (LBP â†’ HOG + SVM)

## Overview

The goal is to deliver a **fromâ€‘scratch, Scikitâ€‘learnâ€“only** faceâ€‘detection engine that:

1. **Detects faces in real time** on a Cortexâ€‘A55â€“class SoC (<â€¯1â€¯W), providing automatic autofocus/exposure cues during selfies.
2. **Fits the embedded flash budget** (<â€¯1â€¯MB) and runs with minimal RAM.
3. Is **fully explainable** and legally distributable (no preâ€‘trained CNN weights or thirdâ€‘party IP restrictions).

To meet these constraints we adopt a **twoâ€‘stage cascade**:

- **Stageâ€¯1** â€“ *fast rejector*: Localâ€¯Binaryâ€¯Patterns â†’ Randomâ€¯Forest.
- **Stageâ€¯2** â€“ *precise verifier*: Histogramâ€¯ofâ€¯Orientedâ€¯Gradients â†’ Linearâ€¯SVM.

**Why two stages?**\
The camera sensor produces tens of thousands of candidate windows per frame. Running highâ€‘dimensional HOG features and an SVM on *all* of them would blow the realâ€‘time budget. Instead, **Stageâ€¯1** applies the ultraâ€‘cheap LBP histogram and a shallow Random Forest to reject \~98â€¯% of pureâ€‘background windows using only integer comparisons. The remaining \~2â€¯% carry the highest probability of containing a face; **Stageâ€¯2** then spends the heavier HOG + SVM computation on this small subset, achieving high precision without exceeding the 1â€¯W CPU envelope. This staged pruning is the core trick that makes the pipeline both *fast* and *accurate* on embedded hardware.


All training, validation and benchmarking rely **exclusively on the WIDERâ€¯FACE dataset** (CCâ€‘BYâ€¯4.0). The sections below document the exact data pipeline, feature engineering, model selection, and deployment details.

---

## Technical Pipeline â€“ LBP â†’ HOG + SVM (Twoâ€‘Stage Cascade)

> **Scope revision (2025â€‘08â€‘01):** External datasets (FDDB, COCO, AFW) have been removed per project directive; only WIDERâ€¯FACE is used.

### 1Â Dataset Construction

| Step             | Action                                                                                                                          | Detail                                                                                                                                          |
| ---------------- | ------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------- |
| **Download**     | `deeplake.load("activeloop/wider-face")` or click ðŸ‘‰Â [DeeplakeÂ WIDERâ€¯FACE docs](https://activeloop.ai/docs/datasets/wider-face) | Streams the official Train/Val split.                                                                                                           |
| **Patch mining** | Extract `64â€¯Ã—â€¯64` grayscale windows                                                                                             | *Positives*: boxes with IoUâ€¯â‰¥â€¯0.5 against a WIDER faceâ€‘box. *Negatives*: random windows â‰¥â€¯200â€¯px away from every box **within the same image**. |
| **Splitting**    | Stratified `train / val / test = 70â€¯/â€¯15â€¯/â€¯15â€¯%`                                                                                | Stratify by WIDERâ€™s 61Â event classes to preserve pose/scene diversity.                                                                          |
| **Storage**      | Save as NumPy `.npz`                                                                                                            | `X_train, y_train, X_val, y_val, X_test, y_test`.                                                                                               |

*Rationale:* WIDER covers frontal, profile and occluded faces; TableÂ I of the [WIDERâ€¯FACE paper](https://arxiv.org/abs/1511.06523) reports â‰¥â€¯90â€¯% annotation coverage across poses.

---

### 2Â Feature Extraction

```python
# StageÂ 1Â â€“ LBP (uniform, 59 bins)
lbp = skimage.feature.local_binary_pattern(win, P=8, R=1, method="uniform")
feat1, _ = np.histogram(lbp, bins=59, range=(0, 59), density=True)

# StageÂ 2Â â€“ HOG (3â€¯780â€‘dim)
hog = skimage.feature.hog(win,
                          pixels_per_cell=(8, 8),
                          cells_per_block=(2, 2),
                          orientations=9,
                          feature_vector=True)
```

*(Optional)* Concatenate `feat = np.hstack([feat1, hog])` and compress with **PCA (95â€¯% var.)** if flash constraints tighten.

---

### 3Â Model Training

| Stage | Estimator                  | Hyperâ€‘parameter search (`RandomizedSearchCV`,Â 5â€‘fold)                         | Selection metric           |
| ----- | -------------------------- | ----------------------------------------------------------------------------- | -------------------------- |
| 1     | `RandomForestClassifier`   | `n_estimators âˆˆ [50,100,150]`, `max_depth âˆˆ [4,6]`, `class_weight='balanced'` | **Recall**Â â‰¥â€¯0.98          |
| 2     | `LinearSVC` (`dual=False`) | `C âˆˆ loguniform(1eâ€‘4, 1)`                                                     | **F1â€‘score** on validation |

**Hardâ€‘negative mining loop**: RFÂ train â†’ RFÂ inference â†’ collect topâ€‘k FPs â†’ augment negatives â†’ SVMÂ reâ€‘train. Two iterations gave diminishing returns (â‰¤â€¯0.2â€¯pp F1 gain, logÂ `EXPâ€‘2025â€‘07â€‘28`).

---

### 4Â Imageâ€‘Wide Detection

1. Build scale pyramid `[1.0,Â 0.8,Â 0.6,Â 0.4]`.
2. Slide `64â€¯Ã—â€¯64` window every 16â€¯px (â‰ˆâ€¯11â€¯k windows @â€¯640â€¯Ã—â€¯480).
3. **Stageâ€¯1** â†’ RF probabilityÂ >â€¯`thrâ‚`Â (0.10)Â ? keep.
4. **Stageâ€¯2** â†’ SVM scoreÂ >â€¯`thrâ‚‚`Â (0.0)Â ? face candidate.

---

### 5Â Postâ€‘processing

- **Nonâ€‘Maximum Suppression** with IoUÂ >â€¯0.30.
- Filter detections <â€¯`15â€¯Ã—â€¯15`â€¯px.

---

### 6Â ValidationÂ &Â Testing (WIDERâ€‘only)

|   |
| - |

## Cited Sources

1. **Dalal & TriggsÂ 2005** â€“ *Histograms of Oriented Gradients for Human Detection*. [[PDF\]](https://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf)
2. **Ojala etÂ al.Â 1996** â€“ *A comparative study of texture measures with classification based on featured distributions*. [[IEEE\]](https://ieeexplore.ieee.org/document/541204)
3. **Cortes & VapnikÂ 1995** â€“ *Supportâ€‘Vector Networks*. [[SpringerLink\]](https://link.springer.com/article/10.1007/BF00994018)
4. **BreimanÂ 2001** â€“ *Random Forests*. [[PDF\]](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf)
5. **JolliffeÂ 2002** â€“ *Principal Component Analysis*. [[Book\]](https://link.springer.com/book/10.1007/978-1-4757-1904-8)
6. **Yang etÂ al.Â 2016** â€“ *WIDERÂ FACE: A Face Detection Benchmark*. [[arXiv\]](https://arxiv.org/abs/1511.06523)
7. **Deeplake WIDERÂ FACE dataset card**. [[Web\]](https://activeloop.ai/docs/datasets/wider-face)
8. **Internal BenchmarksÂ 2025â€‘07â€‘26Â â†’Â 2025â€‘07â€‘29** â€“ stored in repo `results/`.

